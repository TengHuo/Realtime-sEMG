{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a basic LSTM model\n",
    "\n",
    "<p><input type=\"checkbox\", disabled, checked> 1. refactor the processing data code in a utils.py file</p>\n",
    "<p><input type=\"checkbox\", disabled, checked> 2. train a LSTM model</p>\n",
    "<p><input type=\"checkbox\", disabled> 2. train a ConvLSTM model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Dense, Flatten, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import utils, regularizers\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from utils.data import load_capg_data, CapgDBName, LoadMode\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "K.set_session(session)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set const values\n",
    "weights_file = './models/capg_convLSTM_weights_test.h5'\n",
    "model_file = './models/capg_convLSTM_model_test.h5'\n",
    "window_len = 50\n",
    "BATCH_SIZE = 128\n",
    "test_splite = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "(26063, 50, 16, 8, 1)\n",
      "(26063, 8)\n",
      "test set\n",
      "(2880, 50, 16, 8, 1)\n",
      "(2880, 8)\n"
     ]
    }
   ],
   "source": [
    "# load capg dba data\n",
    "x_train, y_train, category = load_capg_data(CapgDBName.dba, LoadMode.sequence_frame)\n",
    "seg_length = int(x_train.shape[1] / window_len)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0]*seg_length, window_len, x_train.shape[2], x_train.shape[3], x_train.shape[4])\n",
    "y_train = y_train.reshape(y_train.shape[0], 1) * np.ones((1, seg_length))\n",
    "y_train = y_train.flatten()\n",
    "y_train = utils.to_categorical(y_train - 1, category)\n",
    "\n",
    "index = np.random.choice(x_train.shape[0], int(x_train.shape[0]*test_splite))\n",
    "x_test = x_train[index]\n",
    "y_test = y_train[index]\n",
    "\n",
    "x_train = np.delete(x_train, index, axis=0)\n",
    "y_train = np.delete(y_train, index, axis=0)\n",
    "\n",
    "print('train set')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print('test set')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = Input(shape=(50, 16, 8, 1), name='input')\n",
    "# x = ConvLSTM2D(filters=32, kernel_size=(3,3), input_shape=(128, 50, 16, 8, 1), padding='same', activation='relu',\n",
    "#                activity_regularizer=regularizers.l1(l=0.01), name='convlstm_1')(inputs)\n",
    "\n",
    "# x = Flatten(name='flat')(x)\n",
    "# predictions = Dense(8, activation='softmax', name='output')(x)\n",
    "# model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/teng/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "load weights from a trained model\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=32, kernel_size=(3,3), input_shape=(50, 16, 8, 1), padding='same',\n",
    "                     activation='relu', activity_regularizer=regularizers.l1(l=0.01), name='convlstm_1'))\n",
    "model.add(Flatten(name='flat'))\n",
    "# model.add(BatchNormalization(momentum=0.9, name='bn_1'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    print('load weights from a trained model')\n",
    "    model.load_weights(model_file, by_name=True)\n",
    "else:\n",
    "    print('train a new model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convlstm_1 (ConvLSTM2D)      (None, 16, 8, 32)         38144     \n",
      "_________________________________________________________________\n",
      "flat (Flatten)               (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 32776     \n",
      "=================================================================\n",
      "Total params: 70,920\n",
      "Trainable params: 70,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(clipnorm=1, clipvalue=0.5, momentum=0.9)\n",
    "# adam = Adam(epsilon=10e-8, clipnorm=1, clipvalue=0.5)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb_callback = TensorBoard(log_dir='./logs/lstm/',\n",
    "#                           histogram_freq=1,\n",
    "#                           batch_size=32,\n",
    "#                           write_grads=True,\n",
    "#                           update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_tuner(epoch):\n",
    "    lr = 0.1\n",
    "    if 16 <= epoch < 40:\n",
    "        lr = 0.01\n",
    "    elif epoch >= 40:\n",
    "        lr = 0.001\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(learning_rate_tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26063 samples, validate on 2880 samples\n",
      "WARNING:tensorflow:From /home/teng/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "26063/26063 [==============================] - 26s 1ms/sample - loss: 2.0696 - acc: 0.1301 - val_loss: 2.0671 - val_acc: 0.1191\n",
      "Epoch 2/60\n",
      "26063/26063 [==============================] - 26s 998us/sample - loss: 2.0654 - acc: 0.1330 - val_loss: 2.0672 - val_acc: 0.1149\n",
      "Epoch 3/60\n",
      "26063/26063 [==============================] - 26s 990us/sample - loss: 2.0635 - acc: 0.1344 - val_loss: 2.0625 - val_acc: 0.1215\n",
      "Epoch 4/60\n",
      "26063/26063 [==============================] - 26s 979us/sample - loss: 2.0440 - acc: 0.1732 - val_loss: 1.9263 - val_acc: 0.2441\n",
      "Epoch 5/60\n",
      "26063/26063 [==============================] - 26s 989us/sample - loss: 1.7931 - acc: 0.3564 - val_loss: 1.6484 - val_acc: 0.4486\n",
      "Epoch 6/60\n",
      "26063/26063 [==============================] - 26s 987us/sample - loss: 1.6099 - acc: 0.4545 - val_loss: 1.5408 - val_acc: 0.4906\n",
      "Epoch 7/60\n",
      "26063/26063 [==============================] - 25s 975us/sample - loss: 1.5006 - acc: 0.5058 - val_loss: 1.4798 - val_acc: 0.5174\n",
      "Epoch 8/60\n",
      "26063/26063 [==============================] - 26s 991us/sample - loss: 1.4552 - acc: 0.5219 - val_loss: 1.4365 - val_acc: 0.5444\n",
      "Epoch 9/60\n",
      "26063/26063 [==============================] - 25s 978us/sample - loss: 1.4314 - acc: 0.5316 - val_loss: 1.3923 - val_acc: 0.5354\n",
      "Epoch 10/60\n",
      "26063/26063 [==============================] - 25s 978us/sample - loss: 1.3911 - acc: 0.5453 - val_loss: 1.3576 - val_acc: 0.5701\n",
      "Epoch 11/60\n",
      "26063/26063 [==============================] - 25s 976us/sample - loss: 1.3773 - acc: 0.5507 - val_loss: 1.3348 - val_acc: 0.5736\n",
      "Epoch 12/60\n",
      "26063/26063 [==============================] - 26s 981us/sample - loss: 1.3410 - acc: 0.5680 - val_loss: 1.3271 - val_acc: 0.5736\n",
      "Epoch 13/60\n",
      "26063/26063 [==============================] - 26s 980us/sample - loss: 1.3348 - acc: 0.5730 - val_loss: 1.3550 - val_acc: 0.5514\n",
      "Epoch 14/60\n",
      "26063/26063 [==============================] - 26s 981us/sample - loss: 1.3162 - acc: 0.5761 - val_loss: 1.3131 - val_acc: 0.5965\n",
      "Epoch 15/60\n",
      "26063/26063 [==============================] - 26s 978us/sample - loss: 1.3072 - acc: 0.5820 - val_loss: 1.2799 - val_acc: 0.6073\n",
      "Epoch 16/60\n",
      "26063/26063 [==============================] - 26s 981us/sample - loss: 1.2736 - acc: 0.5972 - val_loss: 1.2680 - val_acc: 0.6014\n",
      "Epoch 17/60\n",
      "26063/26063 [==============================] - 25s 977us/sample - loss: 1.2411 - acc: 0.6075 - val_loss: 1.2422 - val_acc: 0.6198\n",
      "Epoch 18/60\n",
      "26063/26063 [==============================] - 26s 983us/sample - loss: 1.2248 - acc: 0.6160 - val_loss: 1.2399 - val_acc: 0.6205\n",
      "Epoch 19/60\n",
      "26063/26063 [==============================] - 26s 1ms/sample - loss: 1.2215 - acc: 0.6190 - val_loss: 1.2364 - val_acc: 0.6205\n",
      "Epoch 20/60\n",
      "26063/26063 [==============================] - 26s 986us/sample - loss: 1.2190 - acc: 0.6225 - val_loss: 1.2321 - val_acc: 0.6271\n",
      "Epoch 21/60\n",
      "26063/26063 [==============================] - 25s 962us/sample - loss: 1.2152 - acc: 0.6227 - val_loss: 1.2336 - val_acc: 0.6177\n",
      "Epoch 22/60\n",
      "26063/26063 [==============================] - 25s 968us/sample - loss: 1.2144 - acc: 0.6230 - val_loss: 1.2271 - val_acc: 0.6219\n",
      "Epoch 23/60\n",
      "26063/26063 [==============================] - 25s 963us/sample - loss: 1.2126 - acc: 0.6249 - val_loss: 1.2321 - val_acc: 0.6181\n",
      "Epoch 24/60\n",
      "26063/26063 [==============================] - 25s 978us/sample - loss: 1.2110 - acc: 0.6254 - val_loss: 1.2236 - val_acc: 0.6247\n",
      "Epoch 25/60\n",
      "26063/26063 [==============================] - 26s 996us/sample - loss: 1.2070 - acc: 0.6272 - val_loss: 1.2228 - val_acc: 0.6313\n",
      "Epoch 26/60\n",
      "26063/26063 [==============================] - 25s 965us/sample - loss: 1.2053 - acc: 0.6287 - val_loss: 1.2218 - val_acc: 0.6295\n",
      "Epoch 27/60\n",
      "26063/26063 [==============================] - 26s 998us/sample - loss: 1.2030 - acc: 0.6292 - val_loss: 1.2275 - val_acc: 0.6174\n",
      "Epoch 28/60\n",
      "26063/26063 [==============================] - 25s 972us/sample - loss: 1.2032 - acc: 0.6292 - val_loss: 1.2190 - val_acc: 0.6274\n",
      "Epoch 29/60\n",
      "26063/26063 [==============================] - 26s 983us/sample - loss: 1.2023 - acc: 0.6292 - val_loss: 1.2164 - val_acc: 0.6358\n",
      "Epoch 30/60\n",
      "26063/26063 [==============================] - 25s 966us/sample - loss: 1.1980 - acc: 0.6312 - val_loss: 1.2110 - val_acc: 0.6333\n",
      "Epoch 31/60\n",
      "26063/26063 [==============================] - 26s 983us/sample - loss: 1.1961 - acc: 0.6305 - val_loss: 1.2122 - val_acc: 0.6330\n",
      "Epoch 32/60\n",
      "26063/26063 [==============================] - 25s 975us/sample - loss: 1.1967 - acc: 0.6328 - val_loss: 1.2092 - val_acc: 0.6323\n",
      "Epoch 33/60\n",
      "26063/26063 [==============================] - 25s 977us/sample - loss: 1.1919 - acc: 0.6342 - val_loss: 1.2118 - val_acc: 0.6323\n",
      "Epoch 34/60\n",
      "26063/26063 [==============================] - 25s 968us/sample - loss: 1.1918 - acc: 0.6340 - val_loss: 1.2076 - val_acc: 0.6340\n",
      "Epoch 35/60\n",
      "26063/26063 [==============================] - 25s 969us/sample - loss: 1.1873 - acc: 0.6350 - val_loss: 1.2040 - val_acc: 0.6378\n",
      "Epoch 36/60\n",
      "26063/26063 [==============================] - 26s 994us/sample - loss: 1.1862 - acc: 0.6360 - val_loss: 1.2017 - val_acc: 0.6403\n",
      "Epoch 37/60\n",
      "26063/26063 [==============================] - 26s 1ms/sample - loss: 1.1864 - acc: 0.6369 - val_loss: 1.2043 - val_acc: 0.6351\n",
      "Epoch 38/60\n",
      "26063/26063 [==============================] - 25s 963us/sample - loss: 1.1830 - acc: 0.6376 - val_loss: 1.1988 - val_acc: 0.6448\n",
      "Epoch 39/60\n",
      "26063/26063 [==============================] - 26s 982us/sample - loss: 1.1812 - acc: 0.6378 - val_loss: 1.1964 - val_acc: 0.6448\n",
      "Epoch 40/60\n",
      "26063/26063 [==============================] - 25s 975us/sample - loss: 1.1794 - acc: 0.6385 - val_loss: 1.1980 - val_acc: 0.6462\n",
      "Epoch 41/60\n",
      "26063/26063 [==============================] - 25s 974us/sample - loss: 1.1732 - acc: 0.6430 - val_loss: 1.1940 - val_acc: 0.6441\n",
      "Epoch 42/60\n",
      "26063/26063 [==============================] - 26s 986us/sample - loss: 1.1714 - acc: 0.6428 - val_loss: 1.1934 - val_acc: 0.6420\n",
      "Epoch 43/60\n",
      "26063/26063 [==============================] - 25s 977us/sample - loss: 1.1707 - acc: 0.6437 - val_loss: 1.1934 - val_acc: 0.6403\n",
      "Epoch 44/60\n",
      "26063/26063 [==============================] - 26s 990us/sample - loss: 1.1705 - acc: 0.6430 - val_loss: 1.1930 - val_acc: 0.6375\n",
      "Epoch 45/60\n",
      "26063/26063 [==============================] - 26s 986us/sample - loss: 1.1703 - acc: 0.6429 - val_loss: 1.1927 - val_acc: 0.6424\n",
      "Epoch 46/60\n",
      "26063/26063 [==============================] - 26s 1ms/sample - loss: 1.1702 - acc: 0.6433 - val_loss: 1.1926 - val_acc: 0.6417\n",
      "Epoch 47/60\n",
      "26063/26063 [==============================] - 26s 999us/sample - loss: 1.1697 - acc: 0.6437 - val_loss: 1.1928 - val_acc: 0.6396\n",
      "Epoch 48/60\n",
      "26063/26063 [==============================] - 26s 983us/sample - loss: 1.1697 - acc: 0.6438 - val_loss: 1.1920 - val_acc: 0.6417\n",
      "Epoch 49/60\n",
      "26063/26063 [==============================] - 25s 966us/sample - loss: 1.1694 - acc: 0.6432 - val_loss: 1.1922 - val_acc: 0.6406\n",
      "Epoch 50/60\n",
      "26063/26063 [==============================] - 25s 967us/sample - loss: 1.1692 - acc: 0.6446 - val_loss: 1.1919 - val_acc: 0.6389\n",
      "Epoch 51/60\n",
      "26063/26063 [==============================] - 25s 966us/sample - loss: 1.1690 - acc: 0.6443 - val_loss: 1.1914 - val_acc: 0.6424\n",
      "Epoch 52/60\n",
      "26063/26063 [==============================] - 25s 965us/sample - loss: 1.1686 - acc: 0.6450 - val_loss: 1.1920 - val_acc: 0.6396\n",
      "Epoch 53/60\n",
      "26063/26063 [==============================] - 25s 964us/sample - loss: 1.1687 - acc: 0.6437 - val_loss: 1.1909 - val_acc: 0.6403\n",
      "Epoch 54/60\n",
      " 7680/26063 [=======>......................] - ETA: 18s - loss: 1.1677 - acc: 0.6480"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=60,\n",
    "                    validation_data=(x_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_file)\n",
    "model.save(model_file)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('./figures/convlstm_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('./figures/convlstm_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = '../data/capg_baseline/'\n",
    "# data_files = os.listdir(DATA_DIR)\n",
    "\n",
    "# data_x = np.empty((60800, 50, 8, 16, 3))\n",
    "# data_y = np.zeros((60800, 8))\n",
    "# i = 0\n",
    "# for file_name in data_files:\n",
    "#     if file_name.find('ab-') == -1:\n",
    "#         continue\n",
    "#     gesture_num = int(file_name[5:6])-1\n",
    "#     h5f = h5py.File(os.path.join(DATA_DIR, file_name))\n",
    "#     # 将数据切为50长度的片段\n",
    "#     emg_data = h5f[file_name][:]\n",
    "#     emg_data = emg_data.reshape((emg_data.shape[0]*20,\n",
    "#                                  50,\n",
    "#                                  emg_data.shape[2], \n",
    "#                                  emg_data.shape[3], \n",
    "#                                  emg_data.shape[4]))\n",
    "#     data_x[7600*i:7600*(i+1)] = emg_data\n",
    "#     data_y[7600*i:7600*(i+1), gesture_num] = 1\n",
    "#     i += 1\n",
    "# print(data_x.shape)\n",
    "# print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. 先检查数据输入是否正确\n",
    "# 2. 每个gesture抽样32000个sample进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.1, random_state=42)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print()\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "# del data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
